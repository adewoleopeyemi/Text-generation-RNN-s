# -*- coding: utf-8 -*-
"""AesopFables.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0nkI-37G_CViAX6A1W5ZutcynRjQI4X
"""

import re
import tensorflow.keras as Keras
import numpy as np
Dense = Keras.layers.Dense
LSTM = Keras.layers.LSTM
Input = Keras.Input
Embedding = Keras.layers.Embedding
Dropout = Keras.layers.Dropout
Model = Keras.models.Model

np_utils = Keras.utils

Tokenizer =  Keras.preprocessing.text.Tokenizer



class TextGeneratorRNN:
  def __init__(self, path_to_file = None, seq_length = 10, LSTM_n_units=256, embedding_size = 100, epochs=100, batch_size=32, Dropout_rate=0.2, learning_rate=0.01, temperature = 1, starting_text, length_of_generated_words): 

    self._filename = path_to_file
    self._seq_length = seq_length
    self._LSTM_n_units = LSTM_n_units
    self._embedding_size = embedding_size
    self._epochs = epochs
    self._batch_size = batch_size
    self._Dropout_rate = Dropout_rate
    self._learning_rate = learning_rate
    self._temperature = temperature
    self._starting_text = starting_text
    self._length_of_generated_words = length_of_generated_words

    
  def preprocess_and_tokenize():

    with open(self._filename, encoding='utf-8-sig', errors = 'ignore') as f:
      text = f.read()

    self._start_story = '|'*seq_length
    text = text.lower()
    text = start_story + text
    text = text.replace('\n\n\n\n\n', start_story)
    text = text.replace('\n', ' ')
    text = re.sub('  +', '. ', text).strip()
    text = text.replace('..', '.')

    text = re.sub('([!"#$%&()*+,-./:;<=>?@[\]^_`{|}~])', r' \1 ', text)
    text = re.sub('\s{2,}', ' ', text)

    tokenizer = Tokenizer(char_level = False, filters = '')
    tokenizer.fit_on_texts([text])
    self.total_words = len(tokenizer.word_index)+1
    token_list = tokenizer.texts_to_sequences([text])[0]

    return self.token_list, self.total_words

  
  def generate_sequences(token_list, step):
    self.X_train = []
    self.y_train = []
    for i in range(0, len(token_list)-seq_length, step):
      self.X_train.append(token_list[i: i+seq_length])
      self.y_train.append(token_list[i+seq_length])

    self.y_train = np_utils.to_categorical(y, num_classes=total_words)

    self._num_seq = len(x)

    print('Number of sequneces: ', num_seq, '\n')
    return self.X_train ,self.y_train, self._num_seq


  def build_model(total_words, embedding_size, n_units, dropout, learning_rate):  
    text_in = Input(shape=(None,))\
    x = Embedding(total_words, embedding_size)(text_in)
    x = LSTM(n_units)(x)
    x = Dropout(dropout)(x)
    text_out = Dense(total_words, activation='softmax')(x)

    model = Model(text_in, text_out)
    print(model.summary())

    RMSprop = Keras.optimizers.RMSprop
    opti = RMSprop(learning_rate)

    model.compile(loss='categorical_crossentropy', optimizer=opti)

    return model

  
  def fit_with_text(x, y, epochs, batch_size)
    model.fit(x = x_train, y =y, epochs = epochs, batch_size=batch_size, validation_data=None)
    return model

    
  def sample_with_temp(preds, temperature=1.0):
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds)/temperature
    exp_preds = np.exp(preds)
    preds = exp_preds/np.sum(exp_preds)
    probs = np.random.multinomial(1, preds, 1)
    return np.argmax(probs)

  def generate_text(seed_text, next_words, model, max_sequence_len, temp):
    output_text = seed_text
    seed_text = start_story + seed_text
    print(seed_text)

    for _ in range(next_words):
      token_list = tokenizer.texts_to_sequences([seed_text])[0]
      token_list = token_list[-max_sequence_len:]
      token_list = np.reshape(token_list, (1, max_sequence_len))

      probs = model.predict(token_list, verbose=0)[0]
      y_class = sample_with_temp(probs, temperature=temp)
      output_word = tokenizer.index_word[y_class] if y_class >0 else ''

    if output_word == '|':
      break
    seed_text +=  output_word + ' '
    output_text += ' ' + output_word

  return output_text


  def main():
    token_list, total_words = preprocess_and_tokenize()
    X_train, y_train, num_seq = generate_sequences(token_list, 1)
    X_train = np.array(x)
    y_train = np.array(x)
    model = build_model(total_words, self._embedding_size. self._LSTM_n_units, self._Dropout_rate, self.learning_rate)  
    model = model.fit(X_train, y_train, self._epochs, self._batch_size)
    output_text = generate_text(self._starting_text, 1, self._length_of_generated_words, self._temperature)
    print(output)

  if __name__ == '__main__':
    main()
